\documentclass[../report.tex]{subfiles}

\begin{document}

At its most basic, \emph{Rendering} can be described as ``the process of converting a description of a three-dimensional
scene into an image'' (\cite{pbr-book}). Rendering is used ubiquitously in video games and 3D animation, therefore new techniques are constantly 
being researched to improve the fidelity and performance of rendered scenes.

\emph{Physically Based Rendering} is a type of rendering that attempts to model the way light flows in the real world, and is often employed in the service of 
achieving \emph{Photorealism}, which can be defined loosely as 
``the task of generating images that are indistinguishable from
those that a camera would capture in a photograph or as the task of generating images
that evoke the same response from a human observer as looking at the actual scene.'' (\cite{pbr-book}). 
While the term itself was first popularised in the book ``Physically Based Rendering: From Theory to Implementation'', the general concept has been worked on 
for much longer, such as work done in 1997 at Cornell University (\cite{greenberg}).

Physically based rendering (henceforth referred to as \emph{PBR}) as a technique is used in many modern game engines and modelling software, such as: Blender (\cite{blender-bsdf-docs}) and
Unreal Engine 4 (\cite{ue4-pbr-docs}). PBR often considered an alternative to Rasterisation, which works by converting vector data into less precise groups of pixels, and rendering that to the screen, it is much faster than ray-traced PBR, but less true to life, and is most commonly used in video games, where render speed is of great importance, however many modern game engines are beginning to integrate some features of PBR into their engines to attempt to improve visual fidelity in certain areas.

The aim of this project was to create an application that implements a renderer capable of rendering a pre-made scene using the core techniques that fall under 
the umbrella of PBR, using a low level programming language and API.

In order to facilitate this goal, third party software libraries
were used to implement the sections of the application that were either unrelated to rendering directly or not feasible to implement personally, such as windowing 
(creating an application window at the OS level), linear algebra and hardware access. Regardless of the use of third party libraries, a sizeable amount of setup needs
to be done before rendering code can be written, such as:
\begin{itemize}
    \item Creation and initialisation of the window
    \item Retrieval of surface handle
    \item Main loop setup
    \item Event handling
    \item Graphics pipeline initialisation
    \item Swapchain creation/recreation
\end{itemize}
which are described in later sections.

Because implementing everything that could be considered PBR would have inflated the difficulty of this project considerably, because not only are there 
numerous techniques that would count under PBR, but new techniques are being invented constantly, therefore limiting the scope to only the essential elements 
seemed appropriate, said elements are:
\begin{itemize}
    \item Ray-tracing
    \item Reflections
    \item Shadows
    \item Transparency
    \item Metallics
    \item Bump Mapping
\end{itemize}
With all previously mentioned objectives are implemented correctly, the project will be able to render a fairly convincing image and thus considered to be completed.

For the implementation of the application, we chose to use Rust as the programming language, and used it to interface with the Vulkan API
for rendering graphics, because Vulkan uses a native C interface, it was necessary to use bindings to translate the C API into a Rust one, to this end we elected to use the ash library to provide the bindings.

For the windowing, we decided to use the SDL2 library, partly because of personal familiarity, but also because it allowed me to create a 
cross-platform compatible window that was Vulkan compatible.

Shaders are pieces of code that renderers take as input at runtime and convert directly into code that the GPU can run, these shaders take the information provided to them, and translate it into a format that the GPU can use to produce a rendered image. Vulkan uses a binary shader format called SPIR-V, however writing binary code is inefficient, so SPIR-V is generated by taking GLSL code and converting it to SPIR-V at compile time.
Shaders define how a scene is rendered, which is why the majority of the visuals were implemented via shaders.

In the process of creating the application, a few large tutorials were consulted as a reference on how to set up the project, particularly the Vulkan tutorial \mycite{vulkan-tutorial} and the book of shaders \mycite{book-of-shaders}, as well as the official Vulkan documentation as provided by the Khronos Group.

At the end of the project, we aim to have an application that would be able to render a pre-prepared scene, using some or all of the features that are mentioned above.

\end{document}
